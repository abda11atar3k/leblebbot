from __future__ import annotations

import hashlib
import json
import logging
from typing import Any, Optional

import redis
from groq import Groq

from config import get_settings

logger = logging.getLogger(__name__)


class AIEngine:
    """
    AI Engine with Groq integration, model routing, and semantic caching.
    """

    # Intent categories for routing
    FAST_INTENTS = ["greeting", "faq", "simple_question", "price_inquiry", "thanks"]
    SMART_INTENTS = ["negotiation", "complaint", "complex", "sales", "support", "booking"]

    def __init__(self) -> None:
        self.settings = get_settings()
        self.redis = redis.Redis.from_url(
            self.settings.redis_url, 
            decode_responses=True
        )
        self.groq_client = None
        if self.settings.groq_api_key:
            self.groq_client = Groq(api_key=self.settings.groq_api_key)
        
        self.model_fast = self.settings.ai_model_fast
        self.model_smart = self.settings.ai_model_smart

    def _cache_key(self, text: str) -> str:
        """Generate cache key from text hash"""
        hash_val = hashlib.md5(text.encode()).hexdigest()
        return f"ai_cache:{hash_val}"

    def _get_cached_response(self, prompt: str) -> Optional[str]:
        """Check if response is cached"""
        try:
            cache_key = self._cache_key(prompt)
            cached = self.redis.get(cache_key)
            if cached:
                logger.info("Cache hit for prompt")
                return cached
        except Exception as e:
            logger.warning(f"Redis cache error: {e}")
        return None

    def _cache_response(self, prompt: str, response: str, ttl: int = 3600) -> None:
        """Cache response for future use"""
        try:
            cache_key = self._cache_key(prompt)
            self.redis.setex(cache_key, ttl, response)
        except Exception as e:
            logger.warning(f"Redis cache set error: {e}")

    async def classify_intent(self, message: str) -> dict:
        """
        Classify message intent for routing decisions.
        Returns intent and confidence score.
        """
        if not self.groq_client:
            return {"intent": "unknown", "confidence": 0.5}

        classification_prompt = f"""ØµÙ†Ù‘Ù Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¥Ù„Ù‰ ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø§Ù„ÙØ¦Ø§Øª:
- greeting: ØªØ­ÙŠØ© Ø£Ùˆ Ø³Ù„Ø§Ù…
- faq: Ø³Ø¤Ø§Ù„ Ø¹Ø§Ù…
- price_inquiry: Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ø³Ø¹Ø±
- simple_question: Ø³Ø¤Ø§Ù„ Ø¨Ø³ÙŠØ·
- thanks: Ø´ÙƒØ±
- negotiation: ØªÙØ§ÙˆØ¶ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¹Ø±
- complaint: Ø´ÙƒÙˆÙ‰
- complex: Ø³Ø¤Ø§Ù„ Ù…Ø¹Ù‚Ø¯
- sales: Ù†ÙŠØ© Ø´Ø±Ø§Ø¡
- support: Ø·Ù„Ø¨ Ø¯Ø¹Ù…
- booking: Ø­Ø¬Ø² Ù…ÙˆØ¹Ø¯
- unknown: ØºÙŠØ± ÙˆØ§Ø¶Ø­

Ø§Ù„Ø±Ø³Ø§Ù„Ø©: "{message}"

Ø±Ø¯ Ø¨Ù€ JSON ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù†Øµ Ø¥Ø¶Ø§ÙÙŠ:
{{"intent": "...", "confidence": 0.0-1.0}}"""

        try:
            response = await self._generate_raw(
                classification_prompt, 
                model=self.model_fast,
                max_tokens=100
            )
            # Parse JSON from response
            result = json.loads(response.strip())
            return result
        except Exception as e:
            logger.error(f"Intent classification error: {e}")
            return {"intent": "unknown", "confidence": 0.5}

    def _select_model(self, intent: str, confidence: float) -> str:
        """Select appropriate model based on intent"""
        # Low confidence = use smart model
        if confidence < 0.7:
            return self.model_smart
        
        # Route based on intent
        if intent in self.FAST_INTENTS:
            return self.model_fast
        elif intent in self.SMART_INTENTS:
            return self.model_smart
        else:
            return self.model_fast  # Default to fast

    async def _generate_raw(
        self, 
        prompt: str, 
        model: Optional[str] = None,
        max_tokens: Optional[int] = None
    ) -> str:
        """Generate raw completion from Groq"""
        if not self.groq_client:
            return f"[AI not configured] {prompt[:100]}"

        model = model or self.model_fast
        max_tokens = max_tokens or self.settings.ai_max_tokens

        try:
            response = self.groq_client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.settings.ai_temperature,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Groq API error: {e}")
            return f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."

    async def generate(
        self, 
        message: str, 
        context: dict,
        use_cache: bool = True
    ) -> dict:
        """
        Generate AI response with context.
        Returns response and metadata.
        """
        # Build full prompt
        system_prompt = self._build_system_prompt(context)
        conversation_history = self._format_history(context.get("history", []))
        
        full_prompt = f"{system_prompt}\n\n{conversation_history}\nØ§Ù„Ø¹Ù…ÙŠÙ„: {message}\nØ§Ù„Ù…Ø³Ø§Ø¹Ø¯:"

        # Check cache
        if use_cache:
            cached = self._get_cached_response(full_prompt)
            if cached:
                return {
                    "response": cached,
                    "model": "cache",
                    "cached": True
                }

        # Classify intent for routing
        intent_result = await self.classify_intent(message)
        intent = intent_result.get("intent", "unknown")
        confidence = intent_result.get("confidence", 0.5)

        # Select model
        model = self._select_model(intent, confidence)

        # Generate response
        response = await self._generate_raw(full_prompt, model=model)

        # Cache response
        if use_cache and response:
            self._cache_response(full_prompt, response)

        return {
            "response": response,
            "model": model,
            "intent": intent,
            "confidence": confidence,
            "cached": False
        }

    def _build_system_prompt(self, context: dict) -> str:
        """Build system prompt with business context"""
        business = context.get("business", {})
        user = context.get("user", {})
        knowledge = context.get("knowledge", [])

        bot_name = business.get("bot_name", self.settings.bot_name)
        personality = business.get("personality", self.settings.bot_personality)
        
        dialect_map = {
            "egyptian": "Ø§Ù„Ù…ØµØ±ÙŠØ©",
            "gulf": "Ø§Ù„Ø®Ù„ÙŠØ¬ÙŠØ©",
            "levantine": "Ø§Ù„Ø´Ø§Ù…ÙŠØ©",
            "standard": "Ø§Ù„ÙØµØ­Ù‰"
        }
        dialect = dialect_map.get(
            business.get("dialect", self.settings.bot_dialect), 
            "Ø§Ù„Ù…ØµØ±ÙŠØ©"
        )

        knowledge_text = ""
        if knowledge:
            knowledge_text = "\n\nØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:\n" + "\n".join(
                f"- {k}" for k in knowledge[:5]
            )

        user_info = ""
        if user:
            user_info = f"""
Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¹Ù…ÙŠÙ„:
- Ø§Ù„Ø§Ø³Ù…: {user.get('name', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}
- Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ù„Ø¨Ø§Øª: {user.get('order_count', 0)}
- Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø´ØªØ±ÙŠØ§Øª: {user.get('total_spent', 0)} Ø¬Ù†ÙŠÙ‡
"""

        return f"""Ø£Ù†Øª {bot_name} - Ù…Ø³Ø§Ø¹Ø¯ Ø®Ø¯Ù…Ø© Ø¹Ù…Ù„Ø§Ø¡ Ø°ÙƒÙŠ.

Ø§Ù„Ø´Ø®ØµÙŠØ©: {personality}
Ø§Ù„Ù„Ù‡Ø¬Ø©: {dialect}

Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ù‡Ù…Ø©:
1. Ø±Ø¯ÙˆØ¯Ùƒ Ù‚ØµÙŠØ±Ø© ÙˆÙ…ÙÙŠØ¯Ø© (Ø¬Ù…Ù„Ø© Ø£Ùˆ Ø¬Ù…Ù„ØªÙŠÙ†)
2. Ù„Ø§ ØªÙƒØ±Ø± Ù†ÙØ³Ùƒ
3. Ù„Ø§ ØªØ®ØªØ±Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª - Ù„Ùˆ Ù…Ø´ Ù…ØªØ£ÙƒØ¯ Ù‚ÙˆÙ„ "Ø®Ù„ÙŠÙ†ÙŠ Ø£ØªØ£ÙƒØ¯"
4. Ù„Ùˆ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø²Ø¹Ù„Ø§Ù†ØŒ Ø§Ø¹ØªØ°Ø± ÙˆØ§Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©
5. Ø§Ø³ØªØ®Ø¯Ù… Ø¥ÙŠÙ…ÙˆØ¬ÙŠ Ø¨Ø§Ø¹ØªØ¯Ø§Ù„ ğŸ˜Š
6. Ù„Ùˆ ÙÙŠÙ‡ ÙØ±ØµØ© Ø¨ÙŠØ¹ØŒ Ø§Ù‚ØªØ±Ø­ Ø¨Ø¯ÙˆÙ† Ø¥Ù„Ø­Ø§Ø­
{user_info}{knowledge_text}"""

    def _format_history(self, history: list) -> str:
        """Format conversation history for context"""
        if not history:
            return ""
        
        formatted = []
        for msg in history[-10:]:  # Last 10 messages
            role = "Ø§Ù„Ø¹Ù…ÙŠÙ„" if msg.get("role") == "user" else "Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯"
            content = msg.get("content", "")
            formatted.append(f"{role}: {content}")
        
        return "\n".join(formatted)

    async def detect_sentiment(self, message: str) -> dict:
        """Detect sentiment from message"""
        if not self.groq_client:
            return {"sentiment": "neutral", "score": 0.5}

        prompt = f"""Ø­Ù„Ù„ Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©:
"{message}"

Ø±Ø¯ Ø¨Ù€ JSON ÙÙ‚Ø·:
{{"sentiment": "positive/negative/neutral", "score": 0.0-1.0, "emotions": ["happy", "angry", "frustrated", "satisfied"]}}"""

        try:
            response = await self._generate_raw(prompt, model=self.model_fast, max_tokens=100)
            return json.loads(response.strip())
        except:
            return {"sentiment": "neutral", "score": 0.5, "emotions": []}

    async def extract_entities(self, message: str) -> dict:
        """Extract entities from message (products, prices, dates, etc.)"""
        if not self.groq_client:
            return {"entities": []}

        prompt = f"""Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ø±Ø³Ø§Ù„Ø©:
"{message}"

Ø±Ø¯ Ø¨Ù€ JSON:
{{"entities": [{{"type": "product/price/date/phone/address", "value": "...", "original": "..."}}]}}"""

        try:
            response = await self._generate_raw(prompt, model=self.model_fast, max_tokens=200)
            return json.loads(response.strip())
        except:
            return {"entities": []}

    async def suggest_responses(self, message: str, context: dict) -> list:
        """Generate quick response suggestions"""
        if not self.groq_client:
            return []

        prompt = f"""Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„ØŒ Ø§Ù‚ØªØ±Ø­ 3 Ø±Ø¯ÙˆØ¯ Ø³Ø±ÙŠØ¹Ø© Ù‚ØµÙŠØ±Ø© (ÙƒÙ„ Ø±Ø¯ Ø£Ù‚Ù„ Ù…Ù† 10 ÙƒÙ„Ù…Ø§Øª):
Ø§Ù„Ø±Ø³Ø§Ù„Ø©: "{message}"

Ø±Ø¯ Ø¨Ù€ JSON:
{{"suggestions": ["Ø±Ø¯ 1", "Ø±Ø¯ 2", "Ø±Ø¯ 3"]}}"""

        try:
            response = await self._generate_raw(prompt, model=self.model_fast, max_tokens=150)
            result = json.loads(response.strip())
            return result.get("suggestions", [])
        except:
            return ["Ù‡Ø´ÙˆÙ Ø¯Ù‡ Ù„ÙŠÙƒ", "Ù‡Ø­ÙˆÙ„Ùƒ Ù„Ù…ØªØ®ØµØµ", "ÙÙŠ Ø­Ø§Ø¬Ø© ØªØ§Ù†ÙŠØ© Ø£Ø³Ø§Ø¹Ø¯Ùƒ ÙÙŠÙ‡Ø§ØŸ"]
